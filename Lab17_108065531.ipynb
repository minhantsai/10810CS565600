{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Lab17_108065531.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_9lfYO3Fkhl"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhrqAKe4Fkh6",
        "outputId": "3331fc46-f842-437e-c5c7-ed5220bb1296"
      },
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Restrict TensorFlow to only use the fourth GPU\n",
        "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 Physical GPUs, 1 Logical GPUs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXIqiCXUFkh-",
        "outputId": "f190d3a0-82ed-4ffe-89a8-a9a3601761f0"
      },
      "source": [
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  # this line make pop-out window not appear\n",
        "from ple.games.flappybird import FlappyBird\n",
        "from ple import PLE\n",
        "\n",
        "game = FlappyBird()\n",
        "env = PLE(game, fps=30, display_screen=False)  # environment interface to game\n",
        "env.reset_game()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 1.9.6\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "couldn't import doomish\n",
            "Couldn't import doom\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XhbGyHsFkh_"
      },
      "source": [
        "# Define Input Size\n",
        "IMG_WIDTH = 84\n",
        "IMG_HEIGHT = 84\n",
        "NUM_STACK = 4\n",
        "# For Epsilon-greedy\n",
        "MIN_EXPLORING_RATE = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-19zKIVFkiA"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, name, num_action, discount_factor=0.99):\n",
        "        self.exploring_rate = 0.1\n",
        "        self.discount_factor = discount_factor\n",
        "        self.num_action = num_action\n",
        "        self.model = self.build_model(name)\n",
        "\n",
        "    def build_model(self, name):\n",
        "        # input: state\n",
        "        # output: each action's Q-value \n",
        "        screen_stack = tf.keras.Input(shape=[8,], dtype=tf.float32)\n",
        "        \n",
        "        '''\n",
        "        x = tf.keras.layers.Conv2D(filters=32, kernel_size=8, strides=4)(screen_stack)\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "        x = tf.keras.layers.Conv2D(filters=64, kernel_size=4, strides=2)(x)\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "        x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)(x)\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "        '''\n",
        "        x = tf.keras.layers.Flatten()(screen_stack)\n",
        "        x = tf.keras.layers.Dense(units=512)(x)\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "        Q = tf.keras.layers.Dense(self.num_action)(x)\n",
        "\n",
        "        model = tf.keras.Model(name=name, inputs=screen_stack, outputs=Q)\n",
        "\n",
        "        return model\n",
        "    \n",
        "    def loss(self, state, action, reward, tar_Q, ternimal):\n",
        "        # Q(s,a,theta) for all a, shape (batch_size, num_action)\n",
        "        output = self.model(state)\n",
        "        index = tf.stack([tf.range(tf.shape(action)[0]), action], axis=1)\n",
        "        # Q(s,a,theta) for selected a, shape (batch_size, 1)\n",
        "        Q = tf.gather_nd(output, index)\n",
        "        \n",
        "        # set tar_Q as 0 if reaching terminal state\n",
        "        tar_Q *= ~np.array(terminal)\n",
        "\n",
        "        # loss = E[r+max(Q(s',a',theta'))-Q(s,a,theta)]\n",
        "        loss = tf.reduce_mean(tf.square(reward + self.discount_factor * tar_Q - Q))\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def max_Q(self, state):\n",
        "        # Q(s,a,theta) for all a, shape (batch_size, num_action)\n",
        "        output = self.model(state)\n",
        "\n",
        "        # max(Q(s',a',theta')), shape (batch_size, 1)\n",
        "        return tf.reduce_max(output, axis=1)\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        # epsilon-greedy\n",
        "        if np.random.rand() < self.exploring_rate:\n",
        "            action = np.random.choice(self.num_action)  # Select a random action\n",
        "        else:\n",
        "            state = np.expand_dims(state, axis = 0)\n",
        "            # Q(s,a,theta) for all a, shape (batch_size, num_action)\n",
        "            output = self.model(state)\n",
        "\n",
        "            # select action with highest action-value\n",
        "            action = tf.argmax(output, axis=1)[0]\n",
        "\n",
        "        return action\n",
        "    \n",
        "    def update_parameters(self, episode):\n",
        "        self.exploring_rate = max(MIN_EXPLORING_RATE, min(0.5, 0.99**((episode) / 30)))\n",
        "\n",
        "    def shutdown_explore(self):\n",
        "        # make action selection greedy\n",
        "        self.exploring_rate = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqVrh57nFkiJ"
      },
      "source": [
        "# init agent\n",
        "num_action = len(env.getActionSet())\n",
        "\n",
        "# agent for frequently updating\n",
        "online_agent = Agent('online', num_action)\n",
        "\n",
        "# agent for slow updating\n",
        "target_agent = Agent('target', num_action)\n",
        "# synchronize target model's weight with online model's weight\n",
        "target_agent.model.set_weights(online_agent.model.get_weights())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-3OHlGmFkiO"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "average_loss = tf.keras.metrics.Mean(name='loss')\n",
        "\n",
        "@tf.function\n",
        "def train_step(state, action, reward, next_state, ternimal):\n",
        "    # Delayed Target Network\n",
        "    tar_Q = target_agent.max_Q(next_state)\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = online_agent.loss(state, action, reward, tar_Q, ternimal)\n",
        "    gradients = tape.gradient(loss, online_agent.model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, online_agent.model.trainable_variables))\n",
        "    \n",
        "    average_loss.update_state(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weRVeKjXFkiP"
      },
      "source": [
        "class Replay_buffer():\n",
        "    def __init__(self, buffer_size=50000):\n",
        "        self.experiences = []\n",
        "        self.buffer_size = buffer_size\n",
        "\n",
        "    def add(self, experience):\n",
        "        if len(self.experiences) >= self.buffer_size:\n",
        "            self.experiences.pop(0)\n",
        "        self.experiences.append(experience)\n",
        "\n",
        "    def sample(self, size):\n",
        "        \"\"\"\n",
        "        sample experience from buffer\n",
        "        \"\"\"\n",
        "        if size > len(self.experiences):\n",
        "            experiences_idx = np.random.choice(len(self.experiences), size=size)\n",
        "        else:\n",
        "            experiences_idx = np.random.choice(len(self.experiences), size=size, replace=False)\n",
        "\n",
        "        # from all sampled experiences, extract a tuple of (s,a,r,s')\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        states_prime = []\n",
        "        terminal = []\n",
        "        for i in range(size):\n",
        "            states.append(self.experiences[experiences_idx[i]][0])\n",
        "            actions.append(self.experiences[experiences_idx[i]][1])\n",
        "            rewards.append(self.experiences[experiences_idx[i]][2])\n",
        "            states_prime.append(self.experiences[experiences_idx[i]][3])\n",
        "            terminal.append(self.experiences[experiences_idx[i]][4])\n",
        "\n",
        "        return states, actions, rewards, states_prime, terminal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv1HcrQdFkiT"
      },
      "source": [
        "# init buffer\n",
        "buffer = Replay_buffer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDiuUvNqFkiU"
      },
      "source": [
        "import moviepy.editor as mpy\n",
        "\n",
        "def make_anim(images, fps=60, true_image=False):\n",
        "    duration = len(images) / fps\n",
        "\n",
        "    def make_frame(t):\n",
        "        try:\n",
        "            x = images[int(len(images) / duration * t)]\n",
        "        except:\n",
        "            x = images[-1]\n",
        "\n",
        "        if true_image:\n",
        "            return x.astype(np.uint8)\n",
        "        else:\n",
        "            return ((x + 1) / 2 * 255).astype(np.uint8)\n",
        "\n",
        "    clip = mpy.VideoClip(make_frame, duration=duration)\n",
        "    clip.fps = fps\n",
        "    return clip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsH49gX4FkiV"
      },
      "source": [
        "import skimage.transform\n",
        "\n",
        "def preprocess_screen(screen):\n",
        "    screen = skimage.transform.resize(screen, [IMG_WIDTH, IMG_HEIGHT, 1])\n",
        "    return screen\n",
        "\n",
        "def frames_to_state(input_frames):\n",
        "    if(len(input_frames) == 1):\n",
        "        state = np.concatenate(input_frames*4, axis=-1)\n",
        "    elif(len(input_frames) == 2):\n",
        "        state = np.concatenate(input_frames[0:1]*2 + input_frames[1:]*2, axis=-1)\n",
        "    elif(len(input_frames) == 3):\n",
        "        state = np.concatenate(input_frames + input_frames[2:], axis=-1)\n",
        "    else:\n",
        "        state = np.concatenate(input_frames[-4:], axis=-1)\n",
        "\n",
        "    return state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbNlTM51Fkic",
        "outputId": "78c8a50a-9a3a-4304-9b89-9cd7e52ea5a4"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "update_every_iteration = 1000\n",
        "print_every_episode = 500\n",
        "save_video_every_episode = 5000\n",
        "NUM_EPISODE = 20000\n",
        "NUM_EXPLORE = 20\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "iter_num = 0\n",
        "for episode in range(0, NUM_EPISODE + 1):\n",
        "    \n",
        "    # Reset the environment\n",
        "    env.reset_game()\n",
        "    \n",
        "    # record frame\n",
        "    if episode % save_video_every_episode == 0:\n",
        "        frames = [env.getScreenRGB()]\n",
        "    \n",
        "    # input frame\n",
        "    #input_frames = [preprocess_screen(env.getScreenGrayscale())]\n",
        "    \n",
        "    # for every 500 episodes, shutdown exploration to see the performance of greedy action\n",
        "    if episode % print_every_episode == 0:\n",
        "        online_agent.shutdown_explore()\n",
        "    \n",
        "    # the initial state\n",
        "    state = np.asarray(list(game.getGameState().values()))\n",
        "    # cumulate reward for this episode\n",
        "    cum_reward = 0\n",
        "    t = 0\n",
        "    \n",
        "    while not env.game_over():\n",
        "        \n",
        "        #state = frames_to_state(input_frames)\n",
        "        \n",
        "        # feed current state and select an action\n",
        "        action = online_agent.select_action(state)\n",
        "        \n",
        "        # execute the action and get reward\n",
        "        reward = env.act(env.getActionSet()[action])\n",
        "        \n",
        "        # record frame\n",
        "        if episode % save_video_every_episode == 0:\n",
        "            frames.append(env.getScreenRGB())\n",
        "        \n",
        "        # record input frame\n",
        "        #input_frames.append(preprocess_screen(env.getScreenGrayscale()))\n",
        "        \n",
        "        # cumulate reward\n",
        "        cum_reward += reward\n",
        "        \n",
        "        # observe the result\n",
        "        #state_prime = frames_to_state(input_frames)  # get next state\n",
        "        state_prime = np.asarray(list(game.getGameState().values()))  # get next state\n",
        "        \n",
        "        # append experience for this episode\n",
        "        if episode % print_every_episode != 0:\n",
        "            buffer.add((state, action, reward, state_prime, env.game_over()))\n",
        "        \n",
        "        # Setting up for the next iteration\n",
        "        state = state_prime\n",
        "        t += 1\n",
        "        \n",
        "        # update agent\n",
        "        if episode > NUM_EXPLORE and episode % print_every_episode != 0:\n",
        "            iter_num += 1\n",
        "            train_states, train_actions, train_rewards, train_states_prime, terminal = buffer.sample(BATCH_SIZE)\n",
        "            train_states = np.asarray(train_states).reshape(-1, 8)\n",
        "            train_states_prime = np.asarray(train_states_prime).reshape(-1, 8)\n",
        "            \n",
        "            # convert Python object to Tensor to prevent graph re-tracing\n",
        "            train_states = tf.convert_to_tensor(train_states, tf.float32)\n",
        "            train_actions = tf.convert_to_tensor(train_actions, tf.int32)\n",
        "            train_rewards = tf.convert_to_tensor(train_rewards, tf.float32)\n",
        "            train_states_prime = tf.convert_to_tensor(train_states_prime, tf.float32)\n",
        "            terminal = tf.convert_to_tensor(terminal, tf.bool)\n",
        "            \n",
        "            train_step(train_states, train_actions, train_rewards, train_states_prime, terminal)\n",
        "\n",
        "        # synchronize target model's weight with online model's weight every 1000 iterations\n",
        "        if iter_num % update_every_iteration == 0 and episode > NUM_EXPLORE and episode % print_every_episode != 0:\n",
        "            target_agent.model.set_weights(online_agent.model.get_weights())\n",
        "\n",
        "    # update exploring rate\n",
        "    online_agent.update_parameters(episode)\n",
        "    target_agent.update_parameters(episode)\n",
        "\n",
        "    if episode % print_every_episode == 0 and episode > NUM_EXPLORE:\n",
        "        print(\n",
        "            \"[{}] time live:{}, cumulated reward: {}, exploring rate: {}, average loss: {}\".\n",
        "            format(episode, t, cum_reward, online_agent.exploring_rate, average_loss.result()))\n",
        "        average_loss.reset_states()\n",
        "\n",
        "    if episode % save_video_every_episode == 0:  # for every 500 episode, record an animation\n",
        "        clip = make_anim(frames, fps=60, true_image=True).rotate(-90)\n",
        "        clip.write_videofile(\"movie_f/DQN_lab17-{}.webm\".format(episode), fps=60)\n",
        "#         display(clip.ipython_display(fps=60, autoplay=1, loop=1, maxduration=120))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer flatten is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "t:  10%|▉         | 6/63 [00:00<00:01, 55.43it/s, now=None]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Moviepy - Building video movie_f/DQN_lab17-0.webm.\n",
            "Moviepy - Writing video movie_f/DQN_lab17-0.webm\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                            \r"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready movie_f/DQN_lab17-0.webm\n",
            "[500] time live:21, cumulated reward: -5.0, exploring rate: 0.5, average loss: 2.0189623832702637\n",
            "[1000] time live:68, cumulated reward: -4.0, exploring rate: 0.5, average loss: 0.7403801083564758\n",
            "[1500] time live:112, cumulated reward: -3.0, exploring rate: 0.5, average loss: 0.6638811230659485\n",
            "[2000] time live:69, cumulated reward: -4.0, exploring rate: 0.5, average loss: 0.6274430751800537\n",
            "[2500] time live:73, cumulated reward: -4.0, exploring rate: 0.43277903725889943, average loss: 0.639177143573761\n",
            "[3000] time live:134, cumulated reward: -3.0, exploring rate: 0.3660323412732292, average loss: 0.6959187984466553\n",
            "[3500] time live:188, cumulated reward: -1.0, exploring rate: 0.30957986252419073, average loss: 0.6064209342002869\n",
            "[4000] time live:76, cumulated reward: -4.0, exploring rate: 0.26183394327157605, average loss: 0.37130922079086304\n",
            "[4500] time live:715, cumulated reward: 13.0, exploring rate: 0.22145178723886091, average loss: 0.2745005786418915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "t:   2%|▏         | 9/490 [00:00<00:05, 85.62it/s, now=None]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[5000] time live:489, cumulated reward: 7.0, exploring rate: 0.18729769509073985, average loss: 0.21491564810276031\n",
            "Moviepy - Building video movie_f/DQN_lab17-5000.webm.\n",
            "Moviepy - Writing video movie_f/DQN_lab17-5000.webm\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                              \r"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready movie_f/DQN_lab17-5000.webm\n",
            "[5500] time live:376, cumulated reward: 4.0, exploring rate: 0.15841112426184903, average loss: 0.15842793881893158\n",
            "[6000] time live:678, cumulated reward: 12.0, exploring rate: 0.13397967485796172, average loss: 0.14219368994235992\n",
            "[6500] time live:1357, cumulated reward: 30.0, exploring rate: 0.11331624189077398, average loss: 0.10053379088640213\n",
            "[7000] time live:62, cumulated reward: -5.0, exploring rate: 0.09583969128049684, average loss: 0.08995817601680756\n",
            "[7500] time live:2620, cumulated reward: 63.0, exploring rate: 0.08105851616218128, average loss: 0.09024763852357864\n",
            "[8000] time live:1605, cumulated reward: 36.0, exploring rate: 0.0685570138491429, average loss: 0.06021544337272644\n",
            "[8500] time live:191, cumulated reward: -1.0, exploring rate: 0.05798359469728905, average loss: 0.05444995313882828\n",
            "[9000] time live:62, cumulated reward: -5.0, exploring rate: 0.04904089407128572, average loss: 0.05281048268079758\n",
            "[9500] time live:33, cumulated reward: -5.0, exploring rate: 0.04147740932356356, average loss: 0.04456538334488869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "t:   0%|          | 0/8893 [00:00<?, ?it/s, now=None]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10000] time live:8892, cumulated reward: 230.0, exploring rate: 0.03508042658630376, average loss: 0.04173833504319191\n",
            "Moviepy - Building video movie_f/DQN_lab17-10000.webm.\n",
            "Moviepy - Writing video movie_f/DQN_lab17-10000.webm\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                                \r"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready movie_f/DQN_lab17-10000.webm\n",
            "[10500] time live:114, cumulated reward: -3.0, exploring rate: 0.029670038450977102, average loss: 0.03611139580607414\n",
            "[11000] time live:1377, cumulated reward: 30.0, exploring rate: 0.02509408428990297, average loss: 0.030250390991568565\n",
            "[11500] time live:1305, cumulated reward: 28.0, exploring rate: 0.021223870922486707, average loss: 0.028292935341596603\n",
            "[12000] time live:514, cumulated reward: 7.0, exploring rate: 0.017950553275045137, average loss: 0.02600160427391529\n",
            "[12500] time live:4597, cumulated reward: 116.0, exploring rate: 0.015182073244652034, average loss: 0.028001287952065468\n",
            "[13000] time live:29946, cumulated reward: 789.0, exploring rate: 0.012840570676248398, average loss: 0.0243238378316164\n",
            "[13500] time live:10474, cumulated reward: 272.0, exploring rate: 0.010860193639877882, average loss: 0.023722227662801743\n",
            "[14000] time live:2713, cumulated reward: 66.0, exploring rate: 0.01, average loss: 0.019500991329550743\n",
            "[14500] time live:23654, cumulated reward: 622.0, exploring rate: 0.01, average loss: 0.019814377650618553\n",
            "[15000] time live:20551, cumulated reward: 539.0, exploring rate: 0.01, average loss: 0.018345071002840996\n",
            "Moviepy - Building video movie_f/DQN_lab17-15000.webm.\n",
            "Moviepy - Writing video movie_f/DQN_lab17-15000.webm\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                                   \r"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready movie_f/DQN_lab17-15000.webm\n",
            "[15500] time live:17403, cumulated reward: 456.0, exploring rate: 0.01, average loss: 0.014669117517769337\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b28542b6954a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# feed current state and select an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monline_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# execute the action and get reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-957fd63650bb>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# Q(s,a,theta) for all a, shape (batch_size, num_action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m# select action with highest action-value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    706\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    707\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;31m# explicitly take priority.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0mmask_arg_passed_by_framework\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m     \u001b[0minput_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collect_input_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m     if (self._expects_mask_arg and input_masks is not None and\n\u001b[1;32m    755\u001b[0m         not self._call_arg_was_passed('mask', args, kwargs)):\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_collect_input_masks\u001b[0;34m(self, inputs, args, kwargs)\u001b[0m\n\u001b[1;32m   1982\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_collect_input_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1983\u001b[0m     \u001b[0;34m\"\"\"Checks if `mask` argument was passed, else gathers mask from inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1984\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_arg_was_passed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1985\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_call_arg_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_call_arg_was_passed\u001b[0;34m(self, arg_name, args, kwargs, inputs_in_args)\u001b[0m\n\u001b[1;32m   2001\u001b[0m       \u001b[0;31m# Ignore `inputs` arg.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2002\u001b[0m       \u001b[0mcall_fn_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2003\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_fn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2004\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2005\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwzvcyqoFkif"
      },
      "source": [
        "#from moviepy.editor import *\n",
        "#clip = VideoFileClip(\"movie_f/DQN_lab17-15000.webm\")\n",
        "#display(clip.ipython_display(fps=60, autoplay=1, loop=1, maxduration=360))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_QLNY_IFkin"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}